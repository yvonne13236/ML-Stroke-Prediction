---
title: "HA Project"
author: "Luo Yang"
date: "October 10, 2019"
output:
  word_document: default
  pdf_document: default
---
libraries needed
```{r}
#install.packages('DataExplorer') 
library(DataExplorer)
#install.packages("ROSE")
library(ROSE)



#install.packages("installr"); library(installr) # install+load installr
 
#updateR() # updating R.
```


Import data
```{r}
#setwd("C:/LuLu/UT Dallas/school/2019 Fall/Health Care Analytics/Project/healthcare-dataset-stroke-data (1)")
#test<-read.csv("C:/LuLu/UT Dallas/school/2019 Fall/Health Care Analytics/Project/healthcare-dataset-stroke-data (1)/test_2v.csv")
data<-read.csv("C:/LuLu/UT Dallas/school/2019 Fall/Health Care Analytics/Project/healthcare-dataset-stroke-data (1)/train_2v.csv",header=TRUE,na.strings=c("NA",""))

set.seed(5)
```

Deal with missing values
```{r}

#stroke<-data[data$stroke == 1,]

#sum(is.na(stroke))
#140 missing values

#find columns with missing values
#colnames(train)[colSums(is.na(data)) > 0]


#plot
plot_missing(data)






#impute mising value 
data_num<-data[ ,c("id","age","avg_glucose_level","bmi")]
data_cat<-data[ ,c( "gender", "hypertension","heart_disease","ever_married",     
                    "work_type","Residence_type","smoking_status" )]

library(DMwR)
data_cat<-centralImputation(data_cat) 
sum(is.na(data_cat))#Check for NA's

#imputing numerical by KNN Imputation
data_num<-knnImputation(data_num,k=5)
sum(is.na(data_num))#check for NA's

train<-cbind(data_cat,data_num,data$stroke)
colnames(train)[12] <-"stroke"



#get number of rows and columns
dim(train)
#43400, 12

sum(is.na(train))

train$smoking_status <- as.character(train$smoking_status)
train$smoking_status[train$smoking_status == ""] <- "unknown"
train$smoking_status <- as.factor(train$smoking_status)
#train$smoking_status


```

Factorize stroke
```{r}

train$stroke <- as.factor(train$stroke)




```


EDA

```{r}
#Data type
plot_str(train)
#data type of each columns
sapply(train,class)

#show level of factors
levels(train$ever_married)
levels(train$work_type)
levels(train$Residence_type)
levels(train$smoking_status)
levels(train$stroke)


#continuous variables
plot_histogram(train)
plot_density(train)


#Correlation of numeric features 

plot_correlation(train, type = 'continuous','Review.Date')


#categorical variables
plot_bar(train)

#% of male stroke and female

male_stroke <-sum((train$gender=="Male") & (train$stroke==1))/sum(train$stroke==1)
female_stroke <-sum((train$gender=="Female") & (train$stroke==1))/sum(train$stroke==1) 

female_stroke
male_stroke

#Among people who had a storke, there are slightly more females who have stroke than male



#% of married people stroke and non married
married_stroke <-sum((train$ever_married=="Yes") & (train$stroke==1)) /sum(train$stroke==1)
nmarried_stroke <-sum((train$ever_married=="No") & (train$stroke==1)) /sum(train$stroke==1)

married_stroke 
nmarried_stroke 

#89% of people who have had stroke have been married. 

#smoker??

#smoked_stroke <-sum((train$smoking_status=="smokes") & (train$stroke==1)) /sum(train$stroke==1)
#never_smoked_stroke <-sum((train$smoking_status=="never smoked") & (train$stroke==1)) /sum(train$stroke==1)
#formerly_stroke <-sum((train$smoking_status=="formerly smoked") & (train$stroke==1)) /sum(train$stroke==1)


#smoked_stroke
#never_smoked_stroke
#formerly_stroke 

#https://homepage.divms.uiowa.edu/~luke/classes/STAT4580/catone.html
#stroke status with smoke



```
categorical data viz
```{r}
# library
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)
library(dplyr)
library(ggplot2)
library(tidyr)
library(scales)
#install.packages("ggpubr")
library(ggpubr)
#install.packages("hrbrthemes")
theme_set(theme_pubr())
colnames(train)

#stroke imbalanced chart
train1 <- train %>%
  group_by(stroke) %>%
  summarise(counts = n())
train1

train1 <- train1 %>%
  arrange(desc(stroke)) %>%
  mutate(prop = round(counts*100/sum(counts), 1),
         lab.ypos = cumsum(prop) - 0.5*prop)
head(train1, 4)

#
c4 = c("A", "B")

train1 = cbind(train1, c4)
ggplot(train1, aes(x = stroke, y = counts,fill=c4)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) + 
  theme_pubclean()


#pie chart
ggplot(train1, aes(x = "", y = prop, fill = stroke)) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  geom_text(aes(y = lab.ypos, label = prop), color = "white")+
  coord_polar("y", start = 0)+
  ggpubr::fill_palette("jco")+
  theme_void()



#get stroke and non stroke
stroke <- subset(train, stroke == 1)
nostroke<- subset(train, stroke == 0)




library(dplyr)
library(ggplot2)
#library(ggmap) # for theme_nothing


#gender
male <- subset(train, gender == "Male")
female<- subset(train, gender == "Female")

length(which(male$stroke == 1))


#Heart disease and stroke
p9<-ggplot(stroke, aes(x =heart_disease, fill=heart_disease)) +  
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  labs(title="Heart Disease History Among Patients With Stroke",
          x ="Have Heart Disease Or Not", y = "Percentage")+ theme(legend.position = "none")

  
p10<-ggplot(nostroke, aes(x =heart_disease, fill=heart_disease)) +                                                          
  geom_bar(aes(y = (..count..)/sum(..count..)))+
labs(title="Heart Disease History Among Patients Without Stroke",
          x ="Have Heart Disease Or Not", y = "Percentage")+ theme(legend.position = "none")


grid.arrange(p9, p10, nrow=2)



#hypertension and stroke
p11<-ggplot(stroke, aes(x =hypertension, fill=hypertension)) +  
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  labs(title="Hypertension Among Patients With Stroke",
          x ="Have Hypertension Or Not", y = "Percentage")

  
p12<-ggplot(nostroke, aes(x =hypertension, fill=hypertension)) +                                                      
  geom_bar(aes(y = (..count..)/sum(..count..)))+
labs(title="Hypertension Among Patients Without Stroke",
          x ="Have Hypertension Or Not", y = "Percentage")+ theme(legend.position = "none")

grid.arrange(p11, p12, nrow=2)




#smoke and stroke
#stroke
p1<-ggplot(stroke, aes(x =smoking_status, fill=smoking_status)) +  
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  labs(title="Smoking Status Among Patients With Stroke",
          x ="Smoking Status", y = "Percentage")+ theme(legend.position = "none")

  
  

p2<-ggplot(nostroke, aes(x =smoking_status, fill=smoking_status)) +  
  geom_bar(aes(y = (..count..)/sum(..count..)))+
labs(title="Smoking Status Among Patients Without Stroke",
          x ="Smoking Status", y = "Percentage")+ theme(legend.position = "none")


grid.arrange(p1, p2, nrow=2)



#marriage and stroke
#stroke
p3<-ggplot(stroke, aes(x =ever_married, fill=ever_married)) +  
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  labs(title="Marriage History Among Patients With Stroke",
          x ="Ever Married", y = "Percentage")+ theme(legend.position = "none")

  
p4<-ggplot(nostroke, aes(x =ever_married, fill=ever_married)) +  
  geom_bar(aes(y = (..count..)/sum(..count..)))+
labs(title="Marriage History  Among Patients Without Stroke",
          x ="Ever Married", y = "Percentage")+ theme(legend.position = "none")


grid.arrange(p3, p4, nrow=2)



#work type and stroke
p5<-ggplot(stroke, aes(x =work_type, fill=work_type)) +  
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  labs(title="Work Type Among Patients With Stroke",
          x ="Work Type", y = "Percentage")+ theme(legend.position = "none")

  
p6<-ggplot(nostroke, aes(x =work_type, fill=work_type)) +  
  geom_bar(aes(y = (..count..)/sum(..count..)))+
labs(title="Work Type Among Patients Without Stroke",
          x ="Work Type", y = "Percentage")+ theme(legend.position = "none")


grid.arrange(p5, p6, nrow=2)




#Residence type and stroke
p7<-ggplot(stroke, aes(x =Residence_type, fill=Residence_type)) +  
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  labs(title="Residence Type Among Patients With Stroke",
          x ="Residence Type", y = "Percentage")+ theme(legend.position = "none")

  
p8<-ggplot(nostroke, aes(x =Residence_type, fill=Residence_type)) +  
  geom_bar(aes(y = (..count..)/sum(..count..)))+
labs(title="Residence Type Among Patients Without Stroke",
          x ="Residence Type", y = "Percentage")+ theme(legend.position = "none")



grid.arrange(p7, p8, nrow=2)








```


numerical data viz
```{r}
#stroke and age
stroke.age<-aggregate(train[,9:11], list(train$stroke), mean)
stroke.age<-data.frame(stroke.age)
stroke.age


#stroke and age
stroke.age$Group.1<-factor(stroke.age$Group.1)
ggplot(data=stroke.age, aes(x=Group.1, y=age, fill=Group.1)) + 
    geom_bar(colour="black", width=.2, stat="identity") + 
    guides(fill=FALSE) +
    xlab("Stroke") + ylab("Average Age") +
    ggtitle("Average Age Among People With and Without Stroke")+ 
  geom_text(aes(x = Group.1, y = age, label = round(age, 2)))


#stroke and glucose
ggplot(data=stroke.age, aes(x=Group.1, y=avg_glucose_level, fill=Group.1)) + 
    geom_bar(colour="black", width=.2, stat="identity") + 
    guides(fill=FALSE) +
    xlab("Stroke") + ylab("Average Glucose Level") +
    ggtitle("Average Glucose Level Among People With and Without Stroke")+ 
  geom_text(aes(x = Group.1, y = avg_glucose_level, label = round(avg_glucose_level, 2)))


#stroke and bmi
ggplot(data=stroke.age, aes(x=Group.1, y=bmi, fill=Group.1)) + 
    geom_bar(colour="black", width=.2, stat="identity") + 
    guides(fill=FALSE) +
    xlab("Stroke") + ylab("Average BMI") +
    ggtitle("Average BMI Among People With and Without Stroke")+ 
  geom_text(aes(x = Group.1, y = bmi, label = round(bmi, 2)))



```




create train and validation set
```{r}
library(rpart)

train.rose <- ROSE(stroke ~gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data=train, seed=3)$data
table(train$stroke)


#partition train
#install.packages("caret")
set.seed(5)
library(caret)
samp <- createDataPartition(train.rose$stroke,
    p = 0.8, list = FALSE)
train_2<-train.rose[samp,]
val<- train.rose[-samp, ]
dim(train_2)
dim(val)

table(train_2$stroke)


#rose
#use ROSE (random over sampler) to make data more balanced
#SET SEED



#now the data is balance


#use over sampling



#train_over <- ovun.sample(stroke ~gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_2, method = "over",N = 33552)$data
#table(train_over$stroke)

#undersampling

#data_balanced_under <- ovun.sample(stroke ~gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_2, method = "under", N = 33552, seed = 1)$data
#table(data_balanced_under$stroke)

```



 Random Forest

```{r}
#SET SEED
set.seed(5)


#random forest
#install.packages("randomForest")
library(randomForest)

#By default, number of trees is 500 and number variables tried at each split is 2 in this case. Error rate is 11.16%.

# Fine tuning parameters of Random Forest model
#using cutoff threshold 0.982
model2 <- randomForest(stroke ~gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_2, ntree=50,mtry=6,importance = TRUE,cutoff=c(0.5,0.5))
model2

#When we have increased the mtry to 6 from 2, error rate has reduced from 11.16% to 11.9%. We will now predict on the train dataset first and then predict on validation dataset.


# Predicting on train set
predTrain <- predict(model2, train_2, type = "class")
# Checking classification accuracy
table(predTrain, train_2$stroke)  
accuracy_a<-mean(predTrain==train_2$stroke)

roc.curve(train_2$stroke, predTrain, plotit = F)
precision_a <- posPredValue(predTrain,train_2$stroke, positive="1")
recall_a <- sensitivity(predTrain, train_2$stroke, positive="1")
F1_a <- (2 * precision_a * recall_a) / (precision_a + recall_a)


print("training set: ")
cat("accuracy: ", accuracy_a )
cat("precision: ", precision_a)
cat("recall:",recall_a)
cat("F1:",F1_a)
cat("ROC Curve :",1)



# Predicting on test set
predVal <- predict(model2, val, type = "class")


# Model Evaluation
#accuracy
accuracy<-mean(predVal == val$stroke)                    
table(predVal,val$stroke)
#


#roc curve
roc.curve(val$stroke, predVal, plotit = F)

precision <- posPredValue(predVal, val$stroke, positive="1")


recall <- sensitivity(predVal, val$stroke, positive="1")


F1 <- (2 * precision * recall) / (precision + recall)

print("validation set: ")
cat("accuracy: ", accuracy)
cat("precision: ", precision)
cat("recall:",recall)
cat("F1:",F1)
cat("ROC Curve :",0.873)





importance(model2)        
varImpPlot(model2)  



#roc curve

library(ROCR)

rf_roc1<-predict(model2,train,type = "response")## used vif_log_reg
rf_roc1<-as.numeric(rf_roc1)
y1<-as.numeric(train$stroke)
roc_predictions1<-prediction(rf_roc1,y1)



##Now in, ROC curve, two performance measures are observed--
##TPR(True positive rate) and FPR(False positive rate)
## We have to extract those performance measures from ROCR package using performance()

performance_TPR_FPR1<-performance(roc_predictions1,measure = "tpr",x.measure = "fpr")

##plotting the roc CURVE

plot(performance_TPR_FPR1,col=rainbow(10), colorize=T, print.cutoffs.at=seq(0,1,0.05))

## extracting auc score using performance function

perf_auc1<-performance(roc_predictions1,measure = "auc")
perf_auc1






```

RF Model: cutoff threshold of 0.8
```{r}
model3 <- randomForest(stroke ~gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_2, ntree=50,mtry=6,importance = TRUE,cutoff=c(0.8,1-0.8))

model3


# Predicting on train set
predTrain3 <- predict(model3, train_2, type = "class")
# Checking classification accuracy
table(predTrain3, train_2$stroke)  
accuracy_train_2<-mean(predTrain3== train_2$stroke)    



roc.curve(train_2$stroke, predTrain3, plotit = F)
precision_b <- posPredValue(predTrain3,train_2$stroke, positive="1")
recall_b <- sensitivity(predTrain3, train_2$stroke, positive="1")
F1_b <- (2 * precision_b * recall_b) / (precision_b + recall_b)


print("training set: ")
cat("accuracy: ", accuracy_train_2 )
cat("precision: ", precision_b)
cat("recall:",recall_b)
cat("F1:",F1_b)
cat("ROC Curve :",1)






# Predicting on test set
predVal3 <- predict(model3, val, type = "class")


# Model Evaluation
#accuracy
accuracy3<-mean(predVal3 == val$stroke)                    
table(predVal3,val$stroke)
#


#roc curve
roc.curve(val$stroke, predVal3, plotit = F)

precision3 <- posPredValue(predVal3, val$stroke, positive="1")


recall3 <- sensitivity(predVal3, val$stroke, positive="1")


F1_3 <- (2 * precision3 * recall3) / (precision3 + recall3)

cat("accuracy of validation: ", accuracy3)
cat("precision of validation: ", precision3)
cat("recall of validation:",recall3)
cat("F1 of validation:",F1_3)
cat("ROC Curve of validation:",0.706)

#feature importance
importance(model3)        
varImpPlot(model3)  

#roc curve

library(ROCR)

rf_roc<-predict(model3,val,type = "response")## used vif_log_reg
rf_roc<-as.numeric(rf_roc)
y<-as.numeric(val$stroke)
roc_predictions<-prediction(rf_roc,y)
table(val$stroke)


##Now in, ROC curve, two performance measures are observed--
##TPR(True positive rate) and FPR(False positive rate)
## We have to extract those performance measures from ROCR package using performance()

performance_TPR_FPR<-performance(roc_predictions,measure = "tpr",x.measure = "fpr")

##plotting the roc CURVE

plot(performance_TPR_FPR,col=rainbow(10), colorize=T, print.cutoffs.at=seq(0,1,0.05))

## extracting auc score using performance function

perf_auc<-performance(roc_predictions,measure = "auc")
perf_auc

library(pROC)
rf_roc2<-as.data.frame(predict(model3,val,type = "prob"))

rf_roc2$predict <- names(rf_roc2)[apply(rf_roc2, 1, which.max)]
rf_roc2$observed <- val$stroke
colnames(rf_roc2) <- c("no", "yes","observed","predict")
head(rf_roc2)

# 1 ROC curve, mock vs non mock
roc.mock <- roc(ifelse(rf_roc2$observed==1, 1, 0), as.numeric(rf_roc2$yes))
plot(roc.mock, col = "gray60")






```






Parameter Tuning /CV Random forest


```{r}

#grid search


#library(randomForest)
#library(mlbench)
#library(caret)


#metric <- "Accuracy"
#control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
#set.seed(5)
#tunegrid <- expand.grid(.mtry=c(1:15))
#rf_gridsearch <- train(stroke ~gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_2.rose, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
#print(rf_gridsearch)
#plot(rf_gridsearch)







```

decision tree without balancing
```{r}
#train

#partition train
#install.packages("caret")
set.seed(5)
library(caret)
samp <- createDataPartition(train$stroke,
    p = 0.8, list = FALSE)
train_3<-train[samp,]
val_3<- train[-samp, ]
dim(train_3)
dim(val_3)

table(train_3$stroke)

#model
# penalty matrix
penalty.matrix <- matrix(c(0,1,10,0), byrow=TRUE, nrow=2)

tree1 <- rpart(stroke ~gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_3,parms = list(loss = penalty.matrix),method = "class")

tree1

#plot the tree
library(rattle)
library(rpart.plot)
library(RColorBrewer)
# plot mytree
fancyRpartPlot(tree1, caption = NULL)

#prediction
p_tree1 <-predict(tree1, newdata = val_3, type = "class")

p_tree1

#Calculating accuracy
accuracy4<-mean(p_tree1 == val_3$stroke)  
accuracy4
recall4 <- sensitivity(p_tree1, val_3$stroke, positive="1")
recall4
table(p_tree1,val_3$stroke)


#now to set cutoff threshold
p_tree1_p <-as.data.frame(predict(tree1, newdata = val_3, type = "p"))

p_tree1_p$stroke<-ifelse(p_tree1_p$"1" > .8, 1, 0)
p_tree1_p$stroke<-as.factor(p_tree1_p$stroke)

p_tree1_p

#acccuracy
accuracy5<-mean(p_tree1_p$stroke == val_3$stroke) 
accuracy5

recall5 <- sensitivity(p_tree1_p$stroke, val_3$stroke, positive="1")
recall5




```



